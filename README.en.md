[中文版](README.md)

# Mamba CUDA Implementation Analysis

## Basic Form of SSM

This article analyzes and derives the CUDA code for the Mamba model from the paper **"Mamba: Linear-time sequence modeling with selective state spaces"**[1],  attempting to explain why Mamba is computationally efficient. (**Although Mamba2 has been published at ICML and relies mainly on the Triton library for efficient implementations, avoiding CUDA optimization issues, the original Mamba still has significant reference value**).

Before delving into the code, you can refer to Su Jianlin's blog post ["Revisiting SSM (1): Linear Systems and HiPPO Matrices" (In Chinese)](https://spaces.ac.cn/archives/10114) for an introduction to the theory behind Mamba, ultimately leading to the following basic form of the State Space Model (SSM):

$$
\begin{equation}
\begin{aligned}
    x'(t) &= Ax(t) + Bu(t) \\
    y(t) &= Cx(t) + Du(t)
\end{aligned}
\tag{1}
\end{equation}
$$

Physical meaning: $u(t) \in \mathbb{R}^{D}$ is a function used to record information over a period of time, where $t$ is a continuous time variable, i.e., at any moment $t=t_0$, $u(t_0)$ describes the signal at that moment. Based on this, the first line of differential equation (1) introduces the hidden state variable $x(t) \in \mathbb{R}^{N}$, and establishes the relationship between $x(t)$ and $u(t)$ through the predefined matrices $A \in \mathbb{R}^{N\times N}, B \in \mathbb{R}^{N \times D}$ and the equation $x'(t) = Ax(t) + Bu(t)$ (note that $x^\prime(t)$ is the derivative with respect to $t$). The second line of Equation (1) gives $y(t)$ after determining the values of $x(t)$ and $u(t)$, hence no further discussion is needed (**subsequent discussion will only focus on the first line**). In summary: Input $u(t)$, through the hidden variable $x(t)$, outputs $y(t)$.

## Discretization of SSM

The above form of SSM is for continuous variable $ t $, but is not applicable for practical computation, hence needs discretization. This part is referred from ["SSM Discretization Derivation"](https://zhuanlan.zhihu.com/p/680534665), and finally obtaining the following computable iterative form:

$$
\begin{equation}
\begin{aligned}
	x_k &= \bar{A}x_{k-1} + \bar{B}u_k \\
	y_k &= Cx_k + Du_k \\
	\bar{A} &= e^{\Delta A} \\
	\bar{B} &= A^{-1}(e^{\Delta A}-I)B \\
\end{aligned}
\tag{2}
\end{equation}
$$

Here, $ \Delta = t_k - t_{k - 1} \in \mathbb{R}^{1} $, where $ t_k, t_{k - 1} $ are the moments used for sampling during discretization, $ A \in \mathbb{R}^{N\times N}, B \in \mathbb{R}^{N \times D} $ as above, $ I $ is the identity matrix.

Intuitive understanding: $ u_k $ can correspond to a token in natural language, i.e., given a sequence of length $ L $ with $ D $ channels of tokens $ u_1, u_2, \ldots, u_L \in \mathbb{R}^{D} $, SSM first maps this sequence through the iterative form of formula (2) to the hidden states corresponding to each token $ x_k \in \mathbb{R}^{N} $, and then linearly maps to the output $ y_k $. (**For ease of subsequent analysis, the part about $ y_k $ is omitted, as it is just a simple linear combination of $ x_k, u_k $**).

## Mamba SSM Form

Formula (2) is for a fixed-parameter SSM, but for Mamba, it adopts parameters dependent on input changes, hence the following form:

$$
\begin{equation}
\begin{aligned}
	x_k &= \bar{A_k} x_{k-1} + \bar{B_k} u_k \\
	\bar{A_k} &= e^{\Delta_k A} \\
	\bar{B_k} &= A^{-1}(e^{\Delta_k A}-I)B_k \\
\end{aligned}
\tag{3}
\end{equation}
$$

Here, $ \Delta_k = \Delta_k(u_k) \in \mathbb{R}^{1}, B_k = B_k(u_k) \in \mathbb{R}^{N\times D} $ are determined by the input $ u_k $, for example, they can be generated by a simple linear mapping $ \text{Linear}(u_k) $[1].

## Mamba SSM Simplification

Mamba simplifies SSM on two levels in its implementation; first, for the multi-channel $ \bar{B_k} $, each channel is processed completely independently in actual computation, which significantly reduces computational load while also facilitating GPU parallel processing. Therefore, let $ u_k^i \in \mathbb{R}^1 $ denote the $ i $-th channel of $ u_k $, and since the channels are independent, formula (3) is transformed into independent computation for a single channel, with the following form:

$$
\begin{equation}
\begin{alignedx_{k}^{i} &= \bar{A_k^i}x_{k-1}^{i} + \bar{B_k^i}u_{k}^{i} \in \mathbb{R}^N
\end{aligned}
\tag{4}
\end{equation}
$$

Where $ \bar{A_k^i} $ and $ \bar{B_k^i} $ are defined as

$$
\begin{equation}
\begin{aligned}
    \bar{A_k^i} &= \text{diag} ( e^{\Delta_k^i A} ) \in \mathbb{R} ^{N\times N} \\
    \bar{B_k^i} &= \Delta_{k}^{i}\mu _{k}^{i}B_k\in \mathbb{R} ^{N\times 1}\\
\end{aligned}
\tag{5}
\end{equation}
$$

Here, $\text{diag}(x)$ denotes transforming a vector $x$ into a corresponding diagonal matrix with $x$ positioned on the diagonal. All involved variables are presented in the following format:

$$
\begin{equation}
\begin{aligned}
    u_{k}^{i} \in \mathbb{R}^1, x_{k}^{i} \in \mathbb{R}^N, \Delta _{k}^{i} \in \mathbb{R}^1, \mu _{k}^{i} \in \mathbb{R}^1 , A \in \mathbb{R} ^N, B_k \in \mathbb{R} ^{N\times 1}
\end{aligned}
\end{equation}
$$

Given that $ \bar{A_k^i} $ forms a diagonal matrix, each channel of $ x_k^i $ can be computed independently, simplifying the process. Consider the $ j $-th channel of $ x_k^i \in \mathbb{R}^N $; define the variables $ a_k $ as the $ j $-th element of $ e^{\Delta _{k}^{i}A} \in \mathbb{R}^N $; define $ v_k $ as the $ j $-th element of $ x_k^i $; define $ b_k $ as the $ j $-th element of $ \bar{B_k^i} $, hence:

$$
\begin{equation}
\begin{aligned}
    a_k &= [e^{\Delta _{k}^{i}A}]_j \\
    v_k &= [x_k^i]_j \\
    b_k &= [\bar{B_k^i}]_j \\
\end{aligned}
\tag{6}
\end{equation}
$$

Thus, the core iterative formula becomes:

$$
v_k = a_k v_{k-1} + b_k u_{k}^{i}
\tag{7}
$$

To compute all $ v_0, v_1, \ldots, v_L $ and thus implement SSM, the most straightforward method is to start from the initial value $ v_0 $ and iteratively apply formula (7) in a serial loop. However, this method is not efficient, leading to the following parallel computation process.

## Mamba SSM Parallel Computation

Expanding formula (7) yields the following form:

$$
\begin{aligned}
    v_k &= a_k v_{k-1} + b_k u_{k}^{i} \\
    &= a_k (a_{k-1} v_{k-2} + b_{k-1} u_{k-1}^{i}) + b_k u_{k}^{i} \\
    &= a_k a_{k-1} v_{k-2} + a_k b_{k-1} u_{k-1}^{i} + b_k u_{k}^{i} \\
    &\vdots \\
    &= a_k a_{k-1} \cdots a_1 v_0 + \sum_{j=1}^k{\left( \prod_{m=j+1}^k{a_m} \right)} b_j u_{j}^{i} \\
\end{aligned}
\tag{8}
$$

Although complex, formula (8) essentially involves three types of variables: $ a_{m\ldots n} = a_m a_{m-1} \ldots a_n $, $ v_k $, and $ c_j = b_j u_j^i $. Simplifying, the formula becomes:

$$
v_k = a_{k\ldots 0} v_0 + \sum_{j=1}^k a_{k\ldots j+1} c_j
\tag{9}
$$

This inspires the construction of the following operator ( **inferred from Mamba's CUDA code** ):

$$
\begin{aligned}
    \left[ \begin{array}{c}
    a_{k-1} \\
    v_{k-1} \\
\end{array} \right] 
\oplus
\left[ \begin{array}{c}
    a_k \\
    c_k \\
\end{array} \right] &= \left[ \begin{array}{c}
    a_k a_{k-1} \\
    a_k v_{k-1} + c_k \\
\end{array} \right] \\
\end{aligned}
\tag{10}
$$

It is not difficult to prove that this operator $ \oplus $ is associative:

$$
\begin{aligned
    & \left( \left[\begin{array}{c}
    a_{k-2} \\
    v_{k-2} \\
\end{array} \right] \oplus
\left[ \begin{array}{c}
    a_{k-1} \\
    c_{k-1} \\
\end{array} \right] \right) \oplus \left[ \begin{array}{c}
    a_k \\
    c_k \\
\end{array} \right] = \left[ \begin{array}{c}
    a_{k-1} a_{k-2} \\
    a_{k-1} v_{k-2} + c_{k-1} \\
\end{array} \right] \oplus \left[ \begin{array}{c}
    a_k \\
    c_k \\
\end{array} \right]
    = \left[ \begin{array}{c}
    a_k a_{k-1} a_{k-2} \\
    a_k (a_{k-1} v_{k-2} + c_{k-1}) + c_k \\
\end{array} \right] \\
& \left[ \begin{array}{c}
    a_{k-2} \\
    v_{k-2} \\
\end{array} \right] \oplus \left( \left[ \begin{array}{c}
    a_{k-1} \\
    c_{k-1} \\
\end{array} \right] \oplus \left[ \begin{array}{c}
    a_k \\
    c_k \\
\end{array} \right] \right) = \left[ \begin{array}{c}
    a_{k-2} \\
    v_{k-2} \\
\end{array} \right] \oplus \left[ \begin{array}{c


    a_k a_{k-1} \\
    a_k c_{k-1} + c_k \\
\end{array} \right] 
    = \left[ \begin{array}{c}
    a_k a_{k-1} a_{k-2} \\
    (a_k a_{k-1}) v_{k-2} + (a_k c_{k-1} + c_k) \\
\end{array} \right] \\
& \implies \left( \left[ \begin{array}{c}
    a_{k-2} \\
    v_{k-2} \\
\end{array} \right] \oplus
\left[ \begin{array}{c}
    a_{k-1} \\
    c_{k-1} \\
\end{array} \right] \right) \oplus \left[ \begin{array}{c}
    a_k \\
    c_k \\
\end{array} \right] = \left[ \begin{array}{c}
    a_{k-2} \\
    v_{k-2} \\
\end{array} \right] \oplus \left( \left[ \begin{array}{c}
    a_{k-1} \\
    c_{k-1} \\
\end{array} \right] \oplus \left[ \begin{array}{c}
    a_k \\
    c_k \\
\end{array} \right] \right)
\end{aligned}
$$

Further simplifying the notation, define:

$$
\begin{aligned}
& s_k=\left[ \begin{array}{c}
    a_k\\
    v_k\\
\end{array} \right] 
, e_k=\left[ \begin{array}{c}
    a_k\\
    c_k\\
\end{array} \right] 
\end{aligned}
$$

And according to formula (7), the recursive process follows:

$$
\begin{aligned}
    &s_{k-1}\oplus e_k=\left[ \begin{array}{c}
    a_k a_{k-1}\\
    a_k v_{k-1} + c_k\\
\end{array} \right] =\left[ \begin{array}{c}
    a_k a_{k-1}\\
    v_k\\
\end{array} \right]\\
    &s_{k-2}\oplus e_{k-1}\oplus e_k=\left[ \begin{array}{c}
    a_{k-1} a_{k-2}\\
    v_{k-1}\\
\end{array} \right] \oplus e_k=\left[ \begin{array}{c}
    a_k a_{k-1} a_{k-2}\\
    v_k\\
\end{array} \right]\\
    &\vdots\\
    &s_0\oplus e_1\oplus \cdots \oplus e_k=\left[ \begin{array}{c}
    a_k a_{k-1}\cdots a_0\\
    v_k\\
\end{array} \right]\\
\end{aligned}
$$

By the associative property of the operator $ \oplus $, it follows:

$$
\begin{aligned}
    \left[ \begin{array}{c}
    a_k a_{k-1}\cdots a_0\\
    v_k\\
\end{array} \right] &=\left( \cdots \left( \left( s_0\oplus e_1 \right) \oplus e_2 \right) \cdots \oplus e_k \right)\\
    &=s_0\oplus \left( e_1\oplus \cdots \oplus e_k \right) \oplus \left( e_{k+1}\oplus \cdots \oplus e_n \right)\\
\end{aligned}
\tag{11}
$$

This means that the originally serial computation of $ v_k $ can be transformed into a parallel computation, which is essentially a prefix sum computation problem. To compute:

$$
\begin{aligned}
&v_1 = [s_0 \oplus e_1]_2 \\
&v_2 = [s_0 \oplus e_1 \oplus e_2]_2 \\
&\cdots \\
&v_k = [s_0 \oplus e_1 \oplus \cdots \oplus e_k]_2
\end{aligned}
\tag{12}
$$

Where $ [x]_2 $ is the second element of vector $ x $. CUDA has efficient parallel prefix sum implementations available in libraries such as [BlockScan](https://nvidia.github.io/cccl/cub/api/classcub_1_1BlockScan.html) included in the [CUB](https://nvidia.github.io/cccl/cub/index.html) library.

## Parallel Prefix Sum Computation

Parallel computation of prefix sums effectively utilizes the associative property of generalized sum operators (like the $ \oplus $ operator described) to change the order of summation for efficient parallel computation, as depicted:

![Prefix Sum Diagram](figures/prefix-sum.png)

Each row in the diagram represents a loop iteration, where in the $ i $-th iteration, the $ j $-th CUDA thread fetches the corresponding value from the address $ j + 2^i $ and adds it to its corresponding address $ j $, as described in detail in the book "Programming Massively Parallel Processors: A Hands-on Approach" Chapter 11 on prefix sums and the [BlockScan](https://nvidia.github.io/cccl/cub/api/classcub_1_1BlockScan.html) prefix sum function documentation.

## CUDA Code Analysis

To be continued...

## References

**[1] Gu, Albert, and Tri Dao. "Mamba: Linear-time sequence modeling with selective state spaces."**


